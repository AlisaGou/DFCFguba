    def get_page_num(self):
        """获取页码"""
        urls = self.get_guba()
        for url in urls:  # 遍历每个url
            r = requests.get(url.text)  # 获取url链接
            s = etree.HTML(r)  # 解析网页
            page_num = s.xpath('//*[@id="articlelistnew"]/div[87]/span/span/span')  # 获得个股股吧页数
            _ = self
        return 5

    def list_pag(self, url_format):
        """爬取帖子链接，并抓取详情页"""
        _ = self
        page = self.get_page_num()

        for i in range(page):  # 遍历股吧页数
            ua = UserAgent()  # 设置代理
            self.headers['User-Agent'] = ua.random  # 使用fake-Agent随机生成User-Agent，添加到headers
            r = request_get(url_format, headers=self.headers).text

            try:
                s = etree.HTML(r)
                tiezi_url = s.xpath('//*[@id="articlelistnew"]/div/span[3]/a/@href')
                for url in tiezi_url:
                    urls = 'http://guba.eastmoney.com' + url[0:16]
                    for url in urls:
                        self.detail_pag(url)  # 自动抓取详情页
                        print('正在爬取第%s页' % str(i + 1))  # 打印"正在爬取第n页"
                        time.sleep(1)  # 设置爬取网页的时间间隔为2秒

            except Exception as e:
                _ = e
                print(url_format, r)
                traceback.print_exc()
